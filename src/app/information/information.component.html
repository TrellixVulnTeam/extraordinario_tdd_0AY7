<div class="masthead bg-primary text-white text-center mb-5">
  <header>
    <div class="container d-flex align-items-center flex-column pt-5 pb-4">
      <h1 class="masthead-heading text-uppercase mb-0">Rafael Contreras Sanchez</h1>   
      <hr data-content="PUNTO MEDIO">             
      <p class="masthead-subheading font-weight-light mb-0" style="font-size: 14px;">
        <b style="font-size: 20px;">Teoría de la información</b>
      </p>
    </div>
  </header>
  <section class="container text-justify pb-4">
    <p style="font-size: 16px; text-align: justify;">La <b>Teoría de la Información</b> de Claude E. Shannon, es sin duda uno de 
      los avances científicos más importantes del siglo XX. El principal objetivo de esta teoría es el de proporcionar una 
      definición rigurosa de la noción de información que permita cuantificarla.
    </p>
    <p style="font-size: 16px; text-align: justify;">Shannon estableció resultados matemáticos acerca de los recursos que 
      se necesitan para la codificación óptima y para la comunicación libre de errores. Estos resultados tuvieron aplicaciones 
      en diversos campos ligados a la teoría de las comunicaciones, como en la radio, la televisión y la telefonía. Hoy en día 
      se siguen aplicando en diversas disciplinas.
    </p>
    <p style="font-size: 16px; text-align: justify;">Conceptualmente, se puede pensar que la información se almacena 
      o se transmite como variables que pueden tomar diferentes valores. La entropía de una variable es la "cantidad de 
      información" contenida en la variable. Esta cantidad está determinada no solo por la cantidad de valores diferentes 
      que puede tomar la variable. La entropía de Shannon cuantifica la cantidad de información en una variable, 
      proporcionando así la base para una teoría en torno a la noción de información.
    </p>
    <p style="font-size: 16px; text-align: center;">En este proyecto se calcula la fórmula <b>entropía de Shannon</b></p>  
  </section>
</div>